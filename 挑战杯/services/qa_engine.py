from config.settings import settings
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from database.crud import save_chat_record
from database.session import get_db
from typing import AsyncGenerator
from utils.logger import APILogger
from pathlib import Path

logger = APILogger("QaEngine")

class QaEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.offload_folder = Path("offload")  # æ–°å¢å¸è½½ç›®å½•é…ç½®
        self.offload_folder.mkdir(exist_ok=True)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–æ¨¡å‹"""
        try:
            logger.info("æ­£åœ¨åŠ è½½è¯­è¨€æ¨¡å‹...")
            self.tokenizer = AutoTokenizer.from_pretrained(settings.MODEL_NAME)
            self.model = AutoModelForCausalLM.from_pretrained(
                settings.MODEL_NAME,
                torch_dtype=getattr(torch, settings.MODEL_PRECISION),
                device_map="auto",
                #max_memory={0: "12GiB"},  # æ ¹æ®æ‚¨çš„GPUè°ƒæ•´
                #offload_folder=None,  # ç¦ç”¨å¸è½½
                
                offload_folder=settings.MODEL_OFFLOAD_FOLDER,
                use_safetensors=settings.USE_SAFETENSORS,
                low_cpu_mem_usage=settings.LOW_CPU_MEM_USAGE
            )
            logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise RuntimeError("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥") from e
    
    async def generate_answer(
        self, 
        question: str,
        user_id: int,
        session_id: str
    ) -> AsyncGenerator[str, None]:
        """ç”Ÿæˆå›ç­”å¹¶ä¿å­˜è®°å½•"""
        print("ğŸ” æ”¶åˆ°é—®é¢˜:", question)
        try:
            # å‡†å¤‡è¾“å…¥
            inputs = self.tokenizer(
                question, 
                return_tensors="pt"
            ).to(self.device)
            print("ğŸ§  Tokenization å®Œæˆ")
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=500,
                    temperature=0.7,
                    top_p=0.9
                )
            print("ğŸ¯ æ¨¡å‹ç”Ÿæˆå®Œæˆ")
            
            answer = self.tokenizer.decode(
                outputs[0], 
                skip_special_tokens=True
            )
            print("ğŸ” ç”Ÿæˆå›ç­”:", answer)
            if not answer.strip():
                print("âš ï¸ æ¨¡å‹è¿”å›äº†ç©ºå›ç­”")
                answer = "ï¼ˆâš ï¸ æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ï¼‰"
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            async for session in get_db():
                await save_chat_record(
                    session=session,
                    user_id=user_id,
                    question=question,
                    answer=answer,
                    session_id=session_id
                )
            
            yield answer
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise